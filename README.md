# Bias-Detector
This project involved collecting data to train and evaluate a model that can detect bias in Planned Parenthood's SexEd chatbot called Roo.

Business Goals: Our team was presented with a problem centered around inconsistencies with accurracy in the Planned Parenthood's chatbot Roo. Laws, norms and opinions change on a daily basis which is what accounts for some of the biased and inaccurate responses. Our goal was to determine what types of biases ChatGPT prompts give based on a given keyword and categorize the types of responses ChatGPT gives on sexual health based on a given keyword (biased/ unbiased responses). This would enhance Planned Parenthood and ChatGPT's user experience by offering accurate and up-to date information. The insights from the project can be used to improve Roo, Planned Parenthood’s in-house chatbot, by ensuring that it is accurate in the information it displays to users and that Planned Parenthood has a good brand reputation meaning that they can be trusted leading to higher client retention rates.

Data Preparation and Understanding: The first few initial weeks our team met to walk through the project problem to ensure everyone understood what we needed to do and where to start. In September, which was the start of the data exploration phrase, we refered to the Planned Parenthood reference excel sheet given at the beginning of our project that had sexual health keywords and types of biases to look for. Each team member took each keyword and came up with 1-2 test prompts. In order to test these prompts for bias we had to plug them into chatgpt's 3.5 model to generate sample responses.Based on the response, we individually looked at if the response presented bias for these categories: misinformation, gender bias, socioeconomic assumptions, slang/language and political/religious bias. For some sample prompts we had to test it with two keywords versus one to see if we could get more insight on any time of bias that may be flagged when a certain keyword is detected. Each member collected about 100-120 samples each which we combined on one excel sheet and that served as our dataframe with roughly 500 samples of data in total. Now that we had our dataframe, we had to prepare our data for model training and evaluation. Our data contained String and in order to build our classification model our team decided Hot one encoding would be best. That way we would only have two options: 1 or 0 (biased or not-biased). After the hot one encoding was complete, we went ahead and create some data visuals using matplotlib. Our data was now ready for model training and testing. 

Modeling and Evaluation: When the team got to the model training phase that's when we realized some of the problems that might arise. Though we had roughly 500 samples of data, most of it ontained unbiased test prompts. Our dataset had lest than 10 example that got flagged for bias. On top of that, the dataset was not balanced between the different types of bias. Most of the prompts were tested for socioeconomic assumptiosn meaning there were more examples of that type of bias versus the other four types. A small training data set means there’ll be underfitting in the subsequent model. In order to get a biased response, a specific/detailed question will most likely be needed which does it time consuming. A major question that came up was "How do we determine if something is biased or not"; we all have different understandings of bias which can therefore lead to a lot of human error in the actual model itself. Thus additional research on developing a set criteria so we can agree on biased responses is important. Seeing as we came to this understanding close to the project dealine we ended up not creating a model because of an overgeneralized dataset. 

What we Learnt and Potential Next Steps: The main thing we took away from this project was the importance of understanding the problem because a lot of time was spent going back and forth between the progress we were trying to make and what we needed to do. That time we lost in the beginning would have been helpful in the end to try and fix the imbalances in our dataset. Other takeaways include: the importance of a good training dataset for modeling, understanding what bias looks like and how to categorize bias and developing standard criteria to help determine bias and what category. All those insights prevented us from training a model on imbalanced data as well bringing up an important question on how our own individual ideas can influence major parts of a problem. 

If we did have more time, a few things the team wanted to try were using Natural Language Processing(NLP) for preprocessing the text/data and trying out building various models using algorithms such as Naive Bayes - used a lot in text classification, which does not need much training data, SVM (Support Vector Machines) - efficient with memory, similarly to Naive Bayes, it does not need much training data or Random Forest Regressor. 



